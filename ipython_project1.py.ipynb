{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "through writing causing bone it or . \n",
      "for time when but of to and such fuzzier he in companies . \n",
      "control to P will How eyedness photography known . \n",
      "and needs disorder Medicine intensity for Georgia it infectious 109 additional be peripheral This may of . \n",
      "long Francisco Urol so a aka two I they give is Re fungals they hard real suspect me scabbed 23.3 by is . \n",
      "a and yielding Hey drug diet time start moderately centers first be . \n",
      "hasn each of anyway feel Carlos arm support to are some , great and it the This Robert PA the for of might I yeast at announced the figure animal . \n",
      "one PL5 is know question had saw exposure even the T was . \n",
      "to guess Announcments take the danger Reinhard support Kusumakar is that in . \n",
      "otherwise . \n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "import random\n",
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "REMOVE_IRRELEVANT_TEXT = 1\n",
    "ADD_SENTENCE_BUUNDARY_TAG = 0\n",
    "DIFFERNTIATE_CAPS = 0\n",
    "REMOVE_BAD_SYMBOLS = 1\n",
    "\n",
    "Bad_symbols = \":;'\\\"#$%&()*+-/<=>@[\\]^_`{|}~<>\\|\\\\\"\n",
    "Ending_symbols = \".!?\"\n",
    "Classification = \"data/data_corrected/classification_task/\"\n",
    "Spelling = \"data/data_corrected/spell_checking_task/\"\n",
    "Types_of_file = {\"atheism\", \"autos\", \"graphics\", \"medicine\", \"motorcycles\", \"religion\", \"space\"}\n",
    "File_counts = 300 #0-299 0 might be invalid\n",
    "\n",
    "\n",
    "\n",
    "def format_file_name(task_type, file_type,file_number,train_docs=\"train_docs\"):\n",
    "    if \"cl\" == task_type:\n",
    "        return Classification + file_type + \"/\" + train_docs + \"/\" + file_type + \"_file{}.txt\".format(file_number)\n",
    "    elif \"sp\" == task_type:\n",
    "        if \"modified\" not in train_docs:\n",
    "            return Spelling+ file_type + \"/\" + train_docs + \"/\" + file_type + \"_file{}.txt\".format(file_number)\n",
    "        else:\n",
    "            return Spelling+ file_type + \"/\" + train_docs + \"/\" + file_type + \"_file{}_modified.txt\".format(file_number)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def read_file(task_type: str, file_type: str, file_number: int, train_docs=\"train_docs\"):\n",
    "    file_name = format_file_name(task_type, file_type, file_number, train_docs)\n",
    "    if os.path.exists(file_name):\n",
    "        with open(file_name) as f:\n",
    "            file_content = f.read()\n",
    "            return file_content\n",
    "    return \"\"\n",
    "\n",
    "def preprocess_content(content: str):\n",
    "    if REMOVE_IRRELEVANT_TEXT:\n",
    "        email_pattern = '\\w+@\\w+\\.\\w+'\n",
    "        content = re.sub(email_pattern, ' ', content)\n",
    "    if REMOVE_BAD_SYMBOLS:\n",
    "        regex = re.compile('[%s]' % re.escape(Bad_symbols))\n",
    "        content = regex.sub(' ', content)\n",
    "    return content\n",
    "    \n",
    "def tokenize(file_content: str):\n",
    "    #return list of words given a str\n",
    "    return word_tokenize(file_content)\n",
    "       \n",
    "def bow(tokens: [str]):\n",
    "    #return a dict of word tokens associated with counts\n",
    "    d = dict()\n",
    "    for i in tokens:\n",
    "        if i not in d:\n",
    "            d[i] = 1\n",
    "        else:\n",
    "            d[i] += 1\n",
    "    return d\n",
    "\n",
    "def handle_file(task_type: str, file_type: str, file_number: int, train_docs=\"train_docs\"):\n",
    "    #return bag of word representation of a given file\n",
    "    return bow(tokenize(preprocess_content(read_file(task_type, file_type, file_number, train_docs))))\n",
    "\n",
    "def tokenize_file(task_type: str, file_type: str, file_number: int, train_docs=\"train_docs\"):\n",
    "    #return a list of pre-processed tokens given a file\n",
    "    return tokenize(preprocess_content(read_file(task_type, file_type, file_number, train_docs)))\n",
    "\n",
    "def build_unary_model(c: dict):\n",
    "    #given a BoW, convert count into probability\n",
    "    total = sum(c.values())\n",
    "    d = dict(c)\n",
    "    for i in d:\n",
    "        d[i] = d[i] / total\n",
    "    return d\n",
    "\n",
    "def assign_probability_unary(c: dict)->[tuple]:\n",
    "    #given a unary model, assign a lower and upper bound probability to the word. e.g. [0.23,0.34, 'word']\n",
    "    lower_bound = 0\n",
    "    ret = []\n",
    "    for i in c:\n",
    "        ret.append((lower_bound, lower_bound+c[i], i))\n",
    "        lower_bound += c[i]\n",
    "    return ret\n",
    "\n",
    "def unary_random_word_generation(probability: [tuple]):\n",
    "    #return a random word based on probability given probability list\n",
    "    low, high = 0, len(probability) - 1\n",
    "    random_int = random.random()\n",
    "    \n",
    "    while random_int >= probability[-1][1]:\n",
    "        random_int = random.random() #normalize\n",
    "        \n",
    "    while (low <= high):\n",
    "        mid = (low + high) // 2\n",
    "        if probability[mid][0] > random_int:\n",
    "            high = mid - 1\n",
    "        elif probability[mid][1] <= random_int:\n",
    "            low = mid + 1\n",
    "        else:\n",
    "            return probability[mid][2]\n",
    "\n",
    "def unary_random_sentence_generation(task_type, file_type, train_docs=\"train_docs\"):\n",
    "    C = dict()\n",
    "    for i in range(300):\n",
    "        d2 = handle_file(task_type, file_type, i, train_docs)\n",
    "        for i in d2:\n",
    "            if i in C:\n",
    "                C[i] += d2[i]\n",
    "            else:\n",
    "                C[i] = d2[i]\n",
    "    model = build_unary_model(C)\n",
    "    probability = assign_probability_unary(model)\n",
    "    sentence = \"\"\n",
    "    while 1:\n",
    "        word = unary_random_word_generation(probability)\n",
    "        if word in Ending_symbols:\n",
    "            sentence += word + \" \"\n",
    "            return sentence\n",
    "        sentence += word + \" \"\n",
    "\n",
    "def build_bigram_model(tokens: [str]):\n",
    "    #turn list of tokens into bigrams dict of dict\n",
    "    bigrams = list(nltk.bigrams(tokens))\n",
    "    d = dict()\n",
    "    for i, j in bigrams:\n",
    "        if i not in d:\n",
    "            d[i] = {j: 1}\n",
    "        elif j not in d[i]:\n",
    "            d[i][j] = 1\n",
    "        else:\n",
    "            d[i][j] += 1\n",
    "    return d\n",
    "\n",
    "def bigram_update_model_with_new_tokens(d:\"bigram_model\", tokens):\n",
    "    #update current bigram model with new tokens\n",
    "    new = list(nltk.bigrams(tokens))\n",
    "    for i, j in new:\n",
    "        if i not in d:\n",
    "            d[i] = {j: 1}\n",
    "        elif j not in d[i]:\n",
    "            d[i][j] = 1\n",
    "        else:\n",
    "            d[i][j] += 1\n",
    "    return d\n",
    "\n",
    "def bigram_random_sentence_generation(task_type, file_type, train_docs=\"train_docs\"):\n",
    "\n",
    "    #build unary and generate start word\n",
    "    C = dict() #unary dict\n",
    "    d = dict() #d is bigram_model stored as dict of dict\n",
    "    for i in range(300):\n",
    "        #bow(tokenize(preprocess_content(read_file(task_type, file_type, file_number, train_docs))))\n",
    "        _tokens = tokenize_file(task_type, file_type, i, train_docs)\n",
    "        d2 = bow(_tokens)\n",
    "        for i in d2:\n",
    "            if i in C:\n",
    "                C[i] += d2[i]\n",
    "            else:\n",
    "                C[i] = d2[i]\n",
    "        d = bigram_update_model_with_new_tokens(d, _tokens)\n",
    "    \n",
    "    probability_unary_model = assign_probability_unary(build_unary_model(C))\n",
    "    while 1: #make sure does not start with symbol\n",
    "        current_word = unary_random_word_generation(probability_unary_model)\n",
    "        if current_word not in Ending_symbols and current_word != ',':\n",
    "            break\n",
    "    \n",
    "    ret = current_word\n",
    "    while current_word not in Ending_symbols :\n",
    "        unary = build_unary_model(d[current_word])\n",
    "        current_word = unary_random_word_generation(assign_probability_unary(unary))\n",
    "        ret += \" \" + current_word\n",
    "    return ret\n",
    "\n",
    "def n_ugram_random_sentence_generation(n , task_type, file_type, train_docs=\"train_docs\"):\n",
    "    for i in range(n):\n",
    "        print(unary_random_sentence_generation(task_type, file_type))\n",
    "        \n",
    "def n_bigram_random_sentence_generation(n, task_type, file_type, train_docs=\"train_docs\"):\n",
    "    for i in range(n):\n",
    "        print(bigram_random_sentence_generation(task_type,file_type))\n",
    "\n",
    "        \n",
    "n_ugram_random_sentence_generation(10,'sp','medicine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 0.16816614815437653\n"
     ]
    }
   ],
   "source": [
    "task_type = 'sp'\n",
    "file_type = 'religion'\n",
    "C = dict()\n",
    "for i in range(300):\n",
    "    C.update(bow(tokenize(read_file(task_type, file_type, i))))\n",
    "#x = build_unary_model(C)\n",
    "print(C['.'], C['.'] / len(C) * 100)\n",
    "#print(x['.'], x['?'], x['!'], x.get(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bow(tokenize(read_file('sp','religion', '10'))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "sample = preprocess_content(read_file('sp', 'medicine', 10))\n",
    "regex = re.compile('[%s]' % re.escape(Bad_symbols))\n",
    "content = regex.sub(' ', sample)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 1, 3: 4}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1 = {1:1}\n",
    "d2 = {1:2, 3:4}\n",
    "d2.\n",
    "d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "build_bigram_model(list(\"I like you I don you he is ok I do like you he like is ok \".split()))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
